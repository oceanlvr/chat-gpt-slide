# GPT - Improving Language Understanding by Generative Pre-Training


<div>
对于不同类型的NLP下游任务都能受益于预训练阶段的Transformer。下游阶段Transformer不再进行改变，只微调后面的Linear层。
<img src="/gpt1.jpg" w="80%" h="~"/>

</div>

<!--
这里给出了4种较为常见的NLP下游任务。
分类任务：给定一句话，判断其所属类别。（比如正负面评价）
蕴含任务：给定一段话和一个假设，判断前面的话有没有蕴含后面假设的东西。（例如a支付了b薪水,假设是a雇佣b，那么可以说前面的话是支持这个假设的）那就是可以判断这个两个文本的关系分类。是一个分类任务
相似任务：给两句话，问这两个句子是否是相似的。他将两个句子拼接，然后计算后是否相似这个二分类问题
多选择任务：对于N问题每个问题有一个答案，计算每个问题的答案的一个置信度的标量。最后softmax得到这个答案。
需要注意的是这里引入了一些在预训练阶段没有输入过的标记 [start] [extract] [delim] 这些分割符，不过微调的linear会编码他们。
 -->

<style>
h1 {
  font-size: 21px !important;
}
</style>
