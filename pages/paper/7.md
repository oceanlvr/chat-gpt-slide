# GPT3 - Language Models are Few-Shot Learners

<b>GPT-3 模型达到了 1750 亿参数😱😱😱😱</b>。参数来到了恐怖的千亿级别，训练一次成本数百万美元，目前 OpenAI 向微软授权了 GPT-3，并且开发了 Copilot / Notion AI 等等产品。是主要的收费产品。

论文讨论了 few-shot / zero-shot 在下游任务运用时的效果。整天论文(40+)后25页全部为下游任务的实验。

<Transform :scale="0.9">
<div grid="~ cols-2 gap-2">
<div grid="~ cols-2 gap-0">
<img src="/gpt3-qa.jpg" w="100%" h="~"/>
<img src="/gpt3-qa2.jpg" w="100%" h="~"/>
<img src="/articles.jpg" w="100%" h="~"/>

</div>
<div>
<img src="/gpt3.jpg" w="100%" h="~"/>
</div>
</div>
</Transform>


<!-- 
总结下 GPT-1 和 Bert 做的是一个大规模无监督模型，之后在下游任务有监督finetune获得一个比较好的模型。
而GPT-2直接采用扩大参数、数据量做下游任务的 zero-shot，并且取得了能够和无监督+有监督finetune相媲美的效果。
GPT-3则没有2那么激进重新回到了做 few-shot ，重新给下游任务一些样本。

GPT-3 模型达到了 1750 亿参数。GPT3在下游任务运用时候不用任何的微调/不需要梯度/不需要算任何参数更新。
其次，在论文3.9.4节，GPT产生的文章甚至能够生成不错的效果，在 

-->

<style>
h1 {
  font-size: 21px !important;
}
</style>

