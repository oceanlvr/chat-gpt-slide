# GPT2 - Language Models are Unsupervised Multitask Learners

GPT-2 1.5B参数的模型，在规模上远超 GPT / Bert。
GPT-2 通过高质量的数据集达成了 Zero-Shot SOTA 效果，

GPT2 在结构上并没有太大的改进，主要在模型+数据规模和数据质量上下文章。

> 对于某个下游任务: 无监督超大模型 + 无监督超大数据集 >= 有监督小模型 + 有监督小数据集


<div grid="~ cols-2 gap-2">
<div>
<img src="/gpt2-baseline.jpg" w="100%" h="~" />
<img src="/gpt2-pic.jpg" w="100%" h="~" />
<div class="text-sm">

</div>
</div>
<div>

Training Sample:

`Micheal Jordan is the best basketball player in the history` 
For QA Task:
- Q: `who is the best basketball player in the history?`
- A: `Micheal Jordan`
<Button class="mt-4" title="TASK DETAILS" url="https://openai.com/blog/better-language-models/#task5">
<carbon:book/>
</Button>
</div>
</div>

<!-- 
在GPT推出不久后，其实其并没有在收到很高的反响，因为同期Google出品了和其同样定位的模型Bert。
（Bert在试验数据集和模型上更大、同样是语言模型。只是预训练阶段Transform用的是encoder）
Bert性能在诸多下游任务优于GPT，但是GPT做大规模预训练的启发性是影响深远的。

OpenAI在 Bert 再获成功之后推出了 GPT 第二版本来对标Bert。它把模型直接推到了1.5B级别。但是主要卖点并不是性能，他的性能没有比Bert好太多，而是提出了 few-shot 的概念。
回顾下，刚才训练的方法是统一训练一个大模型然后去下游任务下面去微调后面的MLP，这其实还是要训练两次的。
而一类方法是叫多任务学习，就是在训练过程中不只看一个任务的数据集，而是去训练多个任务的数据，从而能够训练好之后在所有任务都能直接用而不用去微调。（NLP并没有很多做法）
也就是说对于一个通用模型训练好之后，你要去适应到一个下游任务，你需要做两件事情、为这个新的下游任务标注若干数据、并且微调模型的参数。还是有一定成本的。

GPT-2引入了zero-shot，即在下游任务中不需要标注数据y，不需要额外微调。
核心思想就是当无监督的预训练在学习样本的过程中，如果当这个模型足够大并且好、数据集也足够大也足够好。
好到什么情况呢？它就足以覆盖所有的有监督任务，也就是说所有的有监督学习都是无监督语言模型的一个子集。

回到模型上来说，上面说的什么意思，就是GPT1里面的分隔符，他在预训练阶段是不会去学习的，但是在下游任务中，这个分隔符就是一个特殊的符号，他会去学习这个符号的意义（不同的下游任务不同）。
但是在GPT2中他就不用单独去学习这个符号了。

例如当模型训练完“Micheal Jordan is the best basketball player in the history”语料的语言模型之后，便也学会了(question：“who is the best basketball player in the history ?”，answer:“Micheal Jordan”)的Q&A任务。
prompt即里面特殊的分隔符. answer the question,(prompt) document

充分利用更大容量的模型，利用无限的无监督的人写好的文章，去编码其中蕴含的语言学知识，以及，人类的知识。很明显NLP已经在往这个方向转向，而这背后隐藏着什么？超级昂贵的GPU计算时间，超大规模GPU机器学习集群，超长的模型训练过程。归根结底一句话：靠烧钱。靠烧钱背后又有两层意思，一个意思是你没钱就玩不起，就会被清除出NLP的竞赛场；另外一个意思是：即使你们公司有钱，但是钱让不让你花在这上面？当然这是另外一个问题。

总而言之，这说明NLP值得一提的技术进展的玩法已经变了，以后游戏规则变成了：土豪大科技公司靠暴力上数据规模，上GPU或者TPU集群，训练好预训练模型发布出来，不断刷出大新闻。通过暴力美学横扫一切，这是土豪端的玩法。而对于大多数人来说，你能做的是在别人放出来的预训练模型上做小修正或者刷应用或者刷各种榜单，逐步走向了应用人员的方向，这是大多数NLP从业者未来几年要面对的dilemma。
-->

<style>
h1 {
  font-size: 21px !important;
}
</style>

