# GPT 系列启示

<Transform :scale="0.9">


- GPT-1 和 Bert 开启了大规模预训练模型在 NLP 的广泛应用，主要范式是 Pretain + task finetune。
- GPT-2 通过堆砌参数到十亿级别暴力出奇迹，在数据集和模型规模更大的情况下达到了不需要根据下游任务再做 finetune，超越大部分few-shot方法，并且发现无/自监督大模型还可以继续开发，
- GPT-3 贯彻执行到底，参数来到了恐怖的千亿级别，训练一次成本数百万美元，甚至已经没办法做 finetune。在下游任务 few-shot/zero-shot/one-shot 性能上部分甚至超过了有监督的 SOTA。

<div class="text-center"><b>大模型+直接完成下游任务 在部分下游任务上已经足够好！</b></div>
<div grid="~ cols-2 gap-2">
<div >
<img src="/gpt3-pic.jpg" w="100%" h="~"/>
</div>
<div>
<img src="/gpt3-pwl.jpg" w="100%" h="~"/>
</div>
</div>
</Transform>



<!-- 
Bert 走向的是亲民路线，GPT 走向的是超大型模型垄断市场的路线（有人称之为 MASS model as serveice）。

GPT-1 和 Bert 开启了大规模预训练模型在 NLP 的广泛应用，主要范式是 Pretain + task finetune。
- GPT-2 通过堆砌参数到billion级别暴力出奇迹，在数据集和模型规模更大的情况下达到了不需要根据下游任务再做 finetune，超越大部分few-shot方法，并且发现无/自监督大模型还可以继续开发，
- GPT-3 贯彻执行到底，参数来到了恐怖的千亿级别，训练一次成本数百万美元，甚至已经没办法做 finetune。在 few-shot/zero-shot/one-shot 性能上部分任务甚至超过了有监督的 SOTA。

即可以说 GPT 用财大气粗搞出了各AI公司大模型的军备竞赛，并且目前虽然成本在逐步上升，但是性能还未能达到瓶颈。大模型+直接完成下游任务 在大部分下游任务上已经足够惊艳！

其次，在3.9.4节，GPT产生的文章甚至能够生成不错的效果，能让人类不知道其

-->

<style>
h1 {
  font-size: 21px !important;
}
</style>

