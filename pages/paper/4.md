# GPT - Improving Language Understanding by Generative Pre-Training

<span text-sm>GPT 希望同样在 NLP 领域能使用大规模预训练模型。在近 10 个 NLP 任务上都取得了 SOTA 的成绩。</span>

主要分为 pre-tain 和 finetune 两个阶段。

<Transform :scale="0.9">
<div grid="~ cols-2 gap-4">
<div border="~" p-2>

<b>pre-tain</b> 语言模型(unsupervised)，给定文本 $\mathcal{U}$ 和 $\Theta$ 模型情况下，预测词 $u_i$ 出现的概率。
$$
L_1(\mathcal{U})=\sum_{i} \log P\left(u_i \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)
$$

其中 $\Theta$ 模型是 `Transformer decoder`
$$
\begin{aligned}
h_0 & =U W_e+W_p \\
h_l & =\operatorname{transformer} \left(h_{l-1}\right) \forall i \in[1, n] \\
P(u) & =\operatorname{softmax}\left(h_n W_e^T\right)
\end{aligned}
$$
其中$U=\left(u_{-k}, \ldots, u_{-1}\right)$。$W_e$是词嵌入矩阵，$W_p$是位置嵌入矩阵。（可学习的）
</div>
<div border="~" p-2>

<b>fine-tune</b> 下游任务(supervised)，给定文本 $\mathcal{X}$ 和对应 $y$ 标签，预测序列的标号 $y$ 的概率。其中 ${h_l^m}$ 来自最后一层 Transformer block 的输出(第m个词过l层)。具体的

$$
P\left(y \mid x^1, \ldots, x^m\right)=\operatorname{softmax} \left(h_l^m W_y\right)
$$

$$
L_2(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^1, \ldots, x^m\right)\\
L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda * L_1(\mathcal{C})
$$

<span @click="$slidev.nav.next" font="mono" class="text-sm float-right px-2 py-1 rounded cursor-pointer bg-black color-white" hover="bg-opacity-90">
Details <carbon:arrow-right class="inline"/>
</span>

</div>
</div>
</Transform>


<!-- 
首先是GPT的背景，GPT为了解决NLP中缺失一个大规模预训练模型的问题。
在CV通常会用一个大模型训练之后在下游任务微调，达到较好的效果，例如ImageNet。在NLP中一直没有一个这个的大型模型。
暂时的原因可以总结为NLP不知道如何统一下游任务的输入输出、如何找到一个在各个下游任务都好的损失函数。
OpenAI提出了 GPT 框架，GPT 分为两个部分，一个是无监督的预训练模型，一个是有监督的下游任务微调。

预训练阶段，简单说就是用Transformer的decoder作为编码器来训练一个语言模型。
解释下语言模型是一个概率模型，标准语言模型是给定词序列填写词。
即语言模型的输入是一个词和一个模型，输出是下一个词。它的任务目的是根据前面的句子填写词。
具体的，编码器用的是 transformer 解码器，首先对 k 个词做投影，投影是编码了位置和词嵌入信息;
然后过n层transformer块;最后通过一个MLP多层连接层，输出词的概率分布。

下游任务阶段，简单说就是用预训练模型输出的语言模型和一个m长的句子作为输入。
得到一个输出标签yhat,和真实的标签y去有监督训练一个分类器。
其中其关键点在于对于不同的下游任务比如词向量、翻译等等，他的预训练阶段产生的Transformer是不变的。
我们接下来详细介绍下这个框架在的做法。
-->


<style>
h1 {
  font-size: 21px !important;
}
</style>
