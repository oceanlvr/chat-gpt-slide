# GPT 系列启示

<Transform :scale="0.9">

- GPT-1 和 Bert 开启了大规模预训练模型在 NLP 的广泛应用，主要范式是 Pretrain + task finetune。
- GPT-2 通过堆砌参数到十亿级别暴力出奇迹，在数据集和模型规模更大的情况下达到了不需要根据下游任务再做 finetune。
- GPT-3 贯彻执行到底，参数来到了恐怖的千亿级别，训练一次成本数百万美元，甚至已经没办法做 finetune。在下游任务 few-shot/zero-shot/one-shot 性能上部分甚至超过了有监督的 SOTA。

<div class="text-left">
一个超大模型+下游任务不微调
<p>大模型+直接完成下游任务 在部分下游任务上已经足够好！（甚至超过了Finetune）,无监督大模型目前还没有遇到涨点的瓶颈!还有在所有任务上涨点的想象空间</p>

<div flex="~ flex-center items-center justify-center">
<img src="/gpt3-pwl.jpg" class="mx-auto" w="45%" h="~"/>
<img src="/gpt3-pic.jpg" class="mx-auto" w="45%" h="~"/>
</div>

<div grid="~ cols-3 gap-0">
</div>

<div>
</div>
</div>

</Transform>

<!-- 
Bert 走向的是亲民路线，GPT 走向的是超大型模型垄断市场的路线（有人称之为 MASS model as serveice）。

GPT-1 和 Bert 开启了大规模预训练模型在 NLP 的广泛应用，主要范式是 Pretrain + task finetune。
- GPT-2 通过堆砌参数到billion级别暴力出奇迹，在数据集和模型规模更大的情况下达到了不需要根据下游任务再做 finetune，超越大部分few-shot方法，并且发现无/自监督大模型还可以继续开发，
- GPT-3 贯彻执行到底，参数来到了恐怖的千亿级别，训练一次成本数百万美元，甚至已经没办法做 finetune。在 few-shot/zero-shot/one-shot 性能上部分任务甚至超过了有监督的 SOTA。

即可以说 GPT 用财大气粗搞出了各AI公司大模型的军备竞赛，并且目前虽然成本在逐步上升，但是性能还未能达到瓶颈。大模型+直接完成下游任务 在大部分下游任务上已经足够惊艳！

-->

<style>
h1 {
  font-size: 21px !important;
}
ul{
  li{
      font-size: 16px !important;
  }
}
</style>
