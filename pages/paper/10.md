# ChatGPT

ChatGPT 是 GPT 3.5 的 RLHF 版本

<div grid="~ cols-2 gap-2">
<div>
<img src="/chatgpt-algo.svg" w="100%" h="~" />
<Button title="DETAILS" url="https://openai.com/blog/chatgpt/" ><carbon:report-data /></Button>
</div>
<div class="text-xs">
问题：GPT-3回答可能是有毒的 / GPT-3不能准确识别人的意图 / 无法判断什么是高质量的回答（语言模型仅仅有BLUE的约束）。

1. 冷启动阶段的监督策略 Fine-tune 模型。人工标注一些 `<prompt, answer>` 来 Fine-tune GPT-3
2. 训练 RM(Reward Model) 模型，这个阶段通过人工标注数据来指导一个强化学习策略。
  > 输入 `<prompt>` 到冷启动模型。产生`<prompt,answer1>,<prompt,answer2>...<prompt,answerK>`数据，之后根据人类标注去排序打分。利用排序结果训练 RM 模型，过程是使用 PPO, pair-wise 训练打分结果。
<!--
(训练得到一个模型，使得输入`<prompt,answer>`，输出结果的质量越好，打分应该越高)
-->
3. 采用强化学习来增强预训练模型的能力，训练一个 <a href="https://openai.com/blog/openai-baselines-ppo/">PPO</a> 。
  > 由冷启动模型来初始化PPO模型的参数。然后，使用PPO模型对于一个新的采样 prompt 生成回答answer，并用上一阶段训练好的RM模型给出answer质量评估的回报分数score，这个回报分数就是RM赋予回答结果的整体reward。reward可以由此产生梯度可以更新PPO模型参数。增强并以产生符合RM标准的高质量回答。



</div>


</div>

<!-- 
第一阶段：冷启动阶段的监督策略模型。靠GPT 3.5本身，尽管它很强，但是它很难理解人类不同类型指令中蕴含的不同意图，也很难判断生成内容是否是高质量的结果。
为了让GPT 3.5初步具备理解指令中蕴含的意图，首先会从测试用户提交的prompt(就是指令或问题)中随机抽取一批，靠专业的标注人员，给出指定prompt的高质量答案，然后用这些人工标注好的<prompt,answer>数据来Fine-tune GPT 3.5模型。
经过这个过程，我们可以认为GPT 3.5初步具备了理解人类prompt中所包含意图，并根据这个意图给出相对高质量回答的能力。

第二阶段：训练回报模型（Reward Model,RM）。这个阶段的主要目的是通过人工标注训练数据，来训练回报模型。
具体而言，随机抽样一批用户提交的prompt(大部分和第一阶段的相同)，使用第一阶段Fine-tune好的冷启动模型，对于每个prompt，由冷启动模型生成K个不同的回答，于是模型产生出了<prompt,answer1>,<prompt,answer2>….<prompt,answerK>数据。之后，标注人员对K个结果按照很多标准（上面提到的相关性、富含信息性、有害信息等诸多标准）综合考虑进行排序，给出K个结果的排名顺序，这就是此阶段人工标注的数据。
接下来，我们准备利用这个排序结果数据来训练回报模型，采取的训练模式其实就是平常经常用到的 pair-wise learning to rank。对于 K 个排序结果，两两组合，形成个训练数据对，ChatGPT 采取 pair-wise loss 来训练 Reward Model。RM 模型接受一个输入<prompt,answer>，给出评价回答质量高低的回报分数 Score。对于一对训练数据<answer1,answer2>，我们假设人工排序中 answer1 排在 answer2 前面，那么 Loss 函数则鼓励 RM 模型对<prompt,answer1>的打分要比<prompt,answer2>的打分要高。
在这个阶段里，首先由冷启动后的监督策略模型为每个prompt产生K个结果，人工根据结果质量由高到低排序，以此作为训练数据，通过pair-wise learning to rank模式来训练回报模型。对于学好的RM模型来说，输入<prompt,answer>，输出结果的质量得分，得分越高说明产生的回答质量越高。

第三阶段：采用强化学习来增强预训练模型的能力。
本阶段无需人工标注数据，而是利用上一阶段学好的RM模型，靠RM打分结果来更新预训练模型参数。具体而言，首先，从用户提交的prompt里随机采样一批新的命令（指的是和第一第二阶段不同的新的prompt，这个其实是很重要的，对于提升LLM模型理解instruct指令的泛化能力很有帮助），且由冷启动模型来初始化PPO模型的参数。然后，对于随机抽取的prompt，使用PPO模型生成回答answer， 并用上一阶段训练好的RM模型给出answer质量评估的回报分数score，这个回报分数就是RM赋予给整个回答（由单词序列构成）的整体reward。有了单词序列的最终回报，就可以把每个单词看作一个时间步，把reward由后往前依次传递，由此产生的策略梯度可以更新PPO模型参数。这是标准的强化学习过程，目的是训练LLM产生高reward的答案，也即是产生符合RM标准的高质量回答。

-->

<style>
h1 {
  font-size: 21px !important;
}
ul{
  li{
      font-size: 16px !important;
  }
}
</style>
